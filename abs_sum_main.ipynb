{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of this code is based on a PyTorch tutorial found here: \n",
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "# Many alterations and important additions have been made for the goal of this thesis by its author\n",
    "# Author: Konstantinos Papadopoulos\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "# switch between CPU and CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import nltk\n",
    "from config import fit_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "df = pd.read_csv('fake_or_real_news.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "df.title = 'sos ' + df.title + ' eos'\n",
    "\n",
    "Y = df.title\n",
    "X = df['text']\n",
    "\n",
    "# split data into random train and test subsets (20% for testing, 80% for training)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = fit_text(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'republicans'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['target_word2idx']['republicans']\n",
    "config['target_idx2word'][79]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GloVe for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewVocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        # how many times a word appears\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        # index\n",
    "        self.n_words = 0  \n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab = NewVocab('input')\n",
    "input_vocab.word2index = config['input_word2idx']\n",
    "input_vocab.index2word = config['input_idx2word']\n",
    "input_vocab.n_words = config['num_input_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vocab = NewVocab('output')\n",
    "output_vocab.word2index = config['target_word2idx']\n",
    "output_vocab.index2word = config['target_idx2word']\n",
    "output_vocab.n_words = config['num_target_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3371, -0.2169, -0.0066, -0.4162, -1.2555, -0.0285, -0.7219, -0.5289,\n",
       "         0.0072,  0.3200,  0.0294, -0.0132,  0.4351,  0.2572,  0.3900, -0.1197,\n",
       "         0.1504,  0.4476,  0.2841,  0.4934,  0.6283,  0.2289, -0.4038,  0.0274,\n",
       "         0.0074,  0.1400,  0.2335,  0.0681,  0.4842, -0.0196, -0.5475, -0.5498,\n",
       "        -0.0341,  0.0080, -0.4306, -0.0190, -0.0857, -0.8112, -0.2108,  0.3778,\n",
       "        -0.3505,  0.1368, -0.5566,  0.1683, -0.2295, -0.1618,  0.6734, -0.4660,\n",
       "        -0.0318, -0.2604, -0.1780,  0.0194,  0.1073,  0.6653, -0.3484,  0.0478,\n",
       "         0.1644,  0.1409,  0.1920, -0.3501,  0.2624,  0.1763, -0.3137,  0.1171,\n",
       "         0.2038,  0.6177,  0.4908, -0.0752, -0.1182,  0.1868,  0.4068,  0.2832,\n",
       "        -0.1629,  0.0384,  0.4379,  0.0882,  0.5905, -0.0535,  0.0388,  0.1820,\n",
       "        -0.2760,  0.3947, -0.2050,  0.1741,  0.1032,  0.2512, -0.3654,  0.3653,\n",
       "         0.2245, -0.9755,  0.0945, -0.1786, -0.3069, -0.5863, -0.1853,  0.0396,\n",
       "        -0.4231, -0.1572,  0.2040,  0.1691,  0.3447, -0.4226,  0.1955,  0.5945,\n",
       "        -0.3053, -0.1063, -0.1905, -0.5854,  0.2136,  0.3841,  0.0915,  0.3835,\n",
       "         0.2907,  0.0245,  0.2844,  0.0637, -0.1548,  0.4003,  0.3154, -0.0371,\n",
       "         0.0634, -0.2709,  0.2516,  0.4710,  0.4956, -0.3640,  0.1037,  0.0461,\n",
       "         0.1656, -0.2902, -0.0669, -0.3088,  0.4826,  0.3097, -0.1115, -0.1033,\n",
       "         0.0286, -0.1358,  0.5292, -0.1408,  0.0918,  0.1313, -0.2094,  0.0223,\n",
       "        -0.0777,  0.0779, -0.0331,  0.1168,  0.3203,  0.3775, -0.7568, -0.1594,\n",
       "         0.1496,  0.4225,  0.0028,  0.2133,  0.0868, -0.0527, -0.4086, -0.1177,\n",
       "         0.0906, -0.2379, -0.1833,  0.1312, -0.5595,  0.0921, -0.0395,  0.1333,\n",
       "         0.4963,  0.2873, -0.1854,  0.0246, -0.4283,  0.0741,  0.0008,  0.2395,\n",
       "         0.2262,  0.0552, -0.0751, -0.2231,  0.2377, -0.4545,  0.2656, -0.1514,\n",
       "        -0.2415, -0.2474,  0.5521,  0.2682,  0.4883, -0.1342, -0.1592,  0.3761,\n",
       "        -0.1983,  0.1670, -0.1537,  0.2456, -0.0925, -0.3026, -0.2949, -0.7492,\n",
       "         1.0567,  0.3797,  0.6931, -0.0317,  0.2159, -0.4074, -0.1526,  0.3230,\n",
       "        -0.1300, -0.5013, -0.4423,  0.0169, -0.0115,  0.0072,  0.1103,  0.2157,\n",
       "        -0.3237, -0.3729, -0.0092, -0.2677,  0.3907,  0.3574, -0.0606,  0.0680,\n",
       "         0.3383,  0.0657,  0.1579,  0.0472,  0.2368, -0.0914,  0.6465, -0.2549,\n",
       "        -0.6794, -0.6975, -0.1015, -0.3625,  0.3697, -0.4130,  0.0827, -0.3505,\n",
       "        -0.1756,  0.0851, -0.5772,  0.5025,  0.5218,  0.0573, -0.7975, -0.3777,\n",
       "         0.7815,  0.2460,  0.6067, -0.2008, -0.3879,  0.4130, -0.1614,  0.0104,\n",
       "         0.4320,  0.0046,  0.2119, -0.2661, -0.0587, -0.5100,  0.2852,  0.0136,\n",
       "        -0.2735,  0.0618, -0.5790, -0.5114,  0.3638,  0.3514, -0.1650, -0.4604,\n",
       "        -0.0647, -0.6831, -0.0474,  0.1586, -0.4729,  0.3397,  0.0012,  0.1602,\n",
       "        -0.5802,  0.1456, -0.9132, -0.3759, -0.3295,  0.5347,  0.1822, -0.5227,\n",
       "        -0.2621, -0.4246, -0.1803,  0.0995, -0.1511, -0.6673,  0.2448, -0.5663,\n",
       "         0.3384,  0.4056,  0.1807,  0.6425])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = vocab.GloVe(name='6B', dim=300) # I selected 100 dimensional array\n",
    "\n",
    "print('Loaded {} words'.format(len(glove.itos))) # itos = index-to-string\n",
    "# returns the 300 dimensional embedding \n",
    "def get_word(word):\n",
    "    return glove.vectors[glove.stoi[word]].to(device) # stoi = string-to-index\n",
    "\n",
    "get_word('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "matrix_len = input_vocab.n_words\n",
    "weights_matrix = Variable(torch.Tensor(np.zeros((matrix_len, glove.dim))))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(input_vocab.index2word):\n",
    "    try: \n",
    "        weights_matrix[i] = Variable(torch.Tensor(glove[input_vocab.index2word[i]]))\n",
    "        words_found += 12\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = Variable(torch.Tensor(np.random.normal(scale=0.6, size=(emb_dim, ))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = output_vocab.n_words\n",
    "output_weights_matrix = Variable(torch.Tensor(np.zeros((matrix_len, glove.dim))))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(output_vocab.index2word):\n",
    "    try: \n",
    "        output_weights_matrix[i] = Variable(torch.Tensor(glove[output_vocab.index2word[i]]))\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        output_weights_matrix[i] = Variable(torch.Tensor(np.random.normal(scale=0.6, size=(emb_dim, ))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing train-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# The files are all in Unicode, to simplify we will turn Unicode\n",
    "# characters to ASCII, make everything lowercase, and trim most\n",
    "# punctuation.\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to:\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s.decode('latin1'))\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.PUNCTUATION = [\n",
    "        (re.compile(r'([:,])([^\\d])'), r' \\1 \\2'),\n",
    "        (re.compile(r'([:,])$'), r' \\1 '),\n",
    "        (re.compile(r'\\.\\.\\.'), r' ... '),\n",
    "        (re.compile(r'\\#\\#\\#\\#'), r' #### '),\n",
    "        (re.compile(r'[;@$%&]'), r' \\g<0> '),\n",
    "        (re.compile(r'([^\\.])(\\.)([\\]\\)}>\"\\']*)\\s*$'), r'\\1 \\2\\3 '),  # Handles the final period.\n",
    "        (re.compile(r'[?!]'), r' \\g<0> '),\n",
    "\n",
    "        (re.compile(r\"([^'])' \"), r\"\\1 ' \"),\n",
    "    ]\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r'\\([^)]*\\)', '', s)\n",
    "    s = re.sub(r'\\[[^)]*\\]', '', s)\n",
    "    s = re.sub('\\d',  r'#', s)\n",
    "    s = re.sub(\"’|‘|”|“|``\",  r\"'\", s)\n",
    "    #s = re.sub(r\"([.!?:&#'])\", r\" \\1\", s)\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?:&#']+\", r\" \", s)\n",
    "    s = tokenizer.tokenize(text=s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm_train = [normalizeString(train) for train in X_train]\n",
    "X_norm_test = [normalizeString(test) for test in X_test]\n",
    "Y_norm_train = [normalizeString(train) for train in Y_train]\n",
    "Y_norm_test = [normalizeString(test) for test in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 50\n",
    "MAX_OUTPUT_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filter_train = [train[:MAX_INPUT_LENGTH] for train in X_norm_train]\n",
    "X_filter_test = [test[:MAX_INPUT_LENGTH] for test in X_norm_test]\n",
    "Y_filter_train = [train[:MAX_OUTPUT_LENGTH] for train in Y_norm_train]\n",
    "Y_filter_test = [test[:MAX_OUTPUT_LENGTH] for test in Y_norm_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vocab.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSample(vocab, sample):\n",
    "    #vocab.word2index[word]\n",
    "    #return [glove.stoi[word] if (word in glove.stoi) else (glove.stoi['unk']) for word in sample.split(' ')]\n",
    "    #print([vocab.word2index[word] if (word in vocab.word2index) else vocab.word2index['UNK'] for word in sample])\n",
    "    return [vocab.word2index[word] for word in sample if word in vocab.word2index]\n",
    "\n",
    "def tensorFromSample(vocab, sample):\n",
    "    indexes = indexesFromSample(vocab, sample)\n",
    "    #indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSample(input_vocab, pair[0])\n",
    "    target_tensor = tensorFromSample(output_vocab, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = [[x,y] for x, y in zip(X_filter_train, Y_filter_train)]\n",
    "test_pairs = [[x,y] for x, y in zip(X_filter_test, Y_filter_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option #1 for Seq2Seq model: Unidirectional RNN with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# The Seq2Seq Model\n",
    "# Encoder\n",
    "# ^^^^^^^^^^^^^^^^^\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # input_size = number of words in vocab, e.g., 400000\n",
    "        # hidden_size = dimension of embedding, e.g., 100\n",
    "        self.hidden_size = hidden_size\n",
    "        # stores embeddings of the dictionary \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        self.embedding.load_state_dict({'weight': weights_matrix})\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        # applies a Gated Recurrent Unit (GRU) RNN\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        #output = self.conv1d(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# Attention Decoder\n",
    "# ^^^^^^^^^^^^^^^^^\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.5, max_length=MAX_INPUT_LENGTH+1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.embedding.load_state_dict({'weight': output_weights_matrix})\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        \n",
    "        # bmm = batched matrix multiplication\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        #print(output.shape, hidden.shape)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "\n",
    "########################## END OF OPTION 1 ##########################\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option #2 for Seq2Seq model: Bidirectional RNN with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# The Seq2Seq Model\n",
    "# Encoder\n",
    "# ^^^^^^^^^^^^^^^^^\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        #input_size = number of words in vocab, e.g., 400000\n",
    "        #hidden_size = dimension of embedding, e.g., 100\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.embedding.load_state_dict({'weight': weights_matrix})\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.bilstm = nn.LSTM(hidden_size, hidden_size//2, bidirectional=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        \n",
    "        output, hidden = self.bilstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_size//2, device=device),\n",
    "                torch.randn(2, 1, self.hidden_size//2, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# Attention Decoder\n",
    "# ^^^^^^^^^^^^^^^^^\n",
    "\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.5, max_length=MAX_OUTPUT_LENGTH+1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.embedding.load_state_dict({'weight': output_weights_matrix})\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length, )\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.bilstm = nn.LSTM(self.hidden_size, self.hidden_size//2, bidirectional=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        #print(embedded[0].size(), hidden[0].unsqueeze(0).size())\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0].view(1, -1)), 1)), dim=1)\n",
    "        \n",
    "        # bmm = batched matrix multiplication\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.bilstm(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_size//2, device=device),\n",
    "                torch.randn(2, 1, self.hidden_size//2, device=device))\n",
    "    \n",
    "\n",
    "########################## END OF OPTION 2 ##########################\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search (additional option for the decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(decoder,decoder_input, decoder_hidden, encoder_outputs, iters = 20, k = 3):\n",
    "    sequences = [[list(), 0.0]]\n",
    "    \n",
    "    # walk over each step in sequence\n",
    "    for d in range(iters):\n",
    "        #print('sequences',sequences)\n",
    "\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        #print(encoder_outputs)\n",
    "\n",
    "        for i in range(len(sequences)):\n",
    "            #print(sequences)\n",
    "            seq, log_probs = sequences[i]\n",
    "            if d >0:\n",
    "                decoder_input = seq[-1]\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                continue\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            #decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(k)\n",
    "            \n",
    "            #decoded_words.append(input_text.index2word[topi.item()]) #glove.itos\n",
    "                #input_text.index2word\n",
    "\n",
    "            #decoder_input = topi.squeeze().detach()\n",
    "            #print(topv)\n",
    "            for j in range(k):\n",
    "                #print(topv[0][j])\n",
    "                candidate = [seq + [topi[0][j]], log_probs + (topv[0][j])]\n",
    "                #print('candidate', candidate)\n",
    "                all_candidates.append(candidate)\n",
    "                \n",
    "            # order all candidates by score\n",
    "            ordered = sorted(all_candidates, key=lambda tup:-tup[1])\n",
    "            #print(ordered)\n",
    "            # select k best\n",
    "            sequences = ordered[:k]\n",
    "    #print(sequences)    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Beam search evaluation\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "\n",
    "def beam_evaluate(encoder, decoder, sentence, max_length=MAX_INPUT_LENGTH+1):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSample(input_vocab, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        best = beam_search(decoder,decoder_input, decoder_hidden, encoder_outputs, 15, 3)\n",
    "        \n",
    "        for i in best[0][0]:\n",
    "            decoded_words.append(output_vocab.index2word[i.item()])\n",
    "\n",
    "        return decoded_words#, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.2\n",
    "\n",
    "def train(input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion,max_length=MAX_INPUT_LENGTH+1):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    #print(input_tensor.size())\n",
    "\n",
    "    #if input_length>20\n",
    "    for ei in range(input_length):\n",
    "        #print(ei)\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "    \n",
    "    else:   \n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# This is a helper function to print time elapsed and estimated time\n",
    "# remaining given the current time and progress %.\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=1000, learning_rate=0.001, reduce = 10000*6):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(filter(lambda p: p.requires_grad,encoder.parameters()), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(filter(lambda p: p.requires_grad,decoder.parameters()), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(train_pairs)) for i in range(n_iters)]#,size=64\n",
    "    \n",
    "    #print(training_pairs[0],training_pairs[1])\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        #print(input_tensor)\n",
    "        #print(target_tensor)\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "        if iter % reduce == 0:\n",
    "            learning_rate /= 2\n",
    "            print('lr was reduced to: ', learning_rate)\n",
    "            encoder_optimizer = optim.Adam(filter(lambda p: p.requires_grad,encoder.parameters()), lr=learning_rate)\n",
    "            decoder_optimizer = optim.Adam(filter(lambda p: p.requires_grad,decoder.parameters()), lr=learning_rate)\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# Plotting results\n",
    "# ^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# Plotting is done with matplotlib, using the array of loss values\n",
    "# ``plot_losses`` saved while training.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# Evaluation\n",
    "# ^^^^^^^^^^\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_INPUT_LENGTH+1):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSample(input_vocab, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        #best = beam_search(decoder,decoder_input, decoder_hidden, encoder_outputs,20,30)\n",
    "        \n",
    "        for di in range(25):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs) \n",
    "            \n",
    "            #decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                if (di)<14:\n",
    "                    pass\n",
    "                decoded_words.append('eos')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_vocab.index2word[topi.item()]) #glove.itos\n",
    "                \n",
    "            #input_text.index2word\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "             \n",
    "            #decoded_words.append(input_text.index2word[i.item()])\n",
    "        return decoded_words#, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, pairs, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', ' '.join(pair[0]))\n",
    "        print('=', ' '.join(pair[1]))\n",
    "        output_words = evaluate(encoder, decoder, pair[0])#, attentions\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300 #glove.dim\n",
    "encoder1 = EncoderRNN(input_vocab.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_vocab.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 10000*3*3, print_every=1000, learning_rate=0.001, reduce=10000*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, train_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge-1 precision recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge(evaluate):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    scores = []\n",
    "    for pair in test_pairs:\n",
    "        candidate  = evaluate(encoder1, attn_decoder1, pair[0])#, attentions\n",
    "        #reference_p = pair[1].copy()\n",
    "        #reference_r = pair[1].copy()\n",
    "\n",
    "        reference_p = list(pair[1])\n",
    "        reference_r = list(pair[1])\n",
    "        \n",
    "        count_p = 0.0\n",
    "        count_r = 0.0\n",
    "\n",
    "        for i in candidate[1:-1]:\n",
    "            if i in reference_p[1:-1]:\n",
    "                count_p+=1.00\n",
    "                reference_p.remove(i)\n",
    "        if len(candidate[1:-1])>0:\n",
    "            precision = count_p/len(candidate[1:-1]) #precision\n",
    "            precisions.append(precision) \n",
    "\n",
    "        for j in reference_r[1:-1]:\n",
    "            if j in candidate[1:-1]:\n",
    "                count_r+=1.00  \n",
    "\n",
    "        recall = count_r/len(reference_r[1:-1]) #recall\n",
    "        recalls.append(recall) \n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating without Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls = rouge(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = np.mean(np.asarray(precisions))\n",
    "print(pr)\n",
    "rec = np.mean(np.asarray(recalls))\n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07852052556381142"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = pr*rec*2/(pr+rec)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.099703657667497"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisions, recalls = rouge(beam_evaluate)\n",
    "pr = np.mean(np.asarray(precisions))\n",
    "print(pr)\n",
    "rec = np.mean(np.asarray(recalls))\n",
    "print(rec)\n",
    "\n",
    "score = pr*rec*2/(pr+rec)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbf1c2dc210>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x248.471 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_input = \"sunday night's public proclamation that sen. ted cruz of texas and ohio gov. john kasich had reached a truce aimed at ceding states to one another was not predicted but also entirely predictable. with his failure to gain any delegates in new york, cruz is no longer able .\"\n",
    "test_input_norm = [normalizeString(test) for test in test_input]  \n",
    "test_input_final = [test[:MAX_INPUT_LENGTH] for test in test_input_norm]\n",
    "\n",
    "output_words, attentions = evaluate(encoder1, attn_decoder1, test_input)\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_text, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_text.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def evaluateAndShowAttention(input_text):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_text)\n",
    "    print('input =', input_text)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_text, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateAndShowAttention(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), 'encoder.pth')\n",
    "torch.save(attn_decoder1.state_dict(), 'attn_decoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1.load_state_dict(torch.load('encoder.pth'))\n",
    "attn_decoder1.load_state_dict(torch.load('attn_decoder.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
